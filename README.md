# Natural-Language-Processing-with-Disaster-Tweets-with-ELECTRA

This repository looks to aboard this [Kaggle Contest](https://www.kaggle.com/competitions/nlp-getting-started/overview). I achieved a score of 0.82 and 281th place from the leaderboard using an ELECTRA base solution which is complex and better than traditional Recursive alternatives such as RNN and LSTMs.

## Code

The overall implementation and testing is in the notebooks folder.

## Trained Models

All the models are in the models folder, this includes the best model in epoch 1 which converges pretty fast and the other two epoch models.

## Tips and Future Works

It is possible to test with a small batch and with other models such as BERT, DistilBERT, RoBERTa or similar looking the improve the score I achieved.